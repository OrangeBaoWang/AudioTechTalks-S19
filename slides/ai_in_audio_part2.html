<!doctype html>
<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>AI in Audio: Part I</title>

		<link rel="stylesheet" href="css/reveal.css">
		<link rel="stylesheet" href="css/theme/minimal.css">

		<!-- Theme used for syntax highlighting of code -->
		<link rel="stylesheet" href="lib/css/zenburn.css">

		<!-- Styling for the slideshow element -->
		<link rel="stylesheet" href="css/slideshow.css">

		<!-- LaTeX support - Woo! -->
		<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async></script>

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>
	</head>
	<body>
		<div class="reveal">
			<div class="slides">
				<section>
					<div class="title">
						<h1 style="margin: 75px 0 0 0;">AI in Audio</h1>
						<h5 style="margin: 0 0 0 0;"> Part 2: Music Information Retrieval</h5>
						<h5 style="margin: 100px 0 0 0;">Spring 2019 - Audio Tech Talk Series</h5>
						<h6 style="margin: 5px 0 0 0;">March 12, 2019<h6>
					</div>
				</section>

				<!-- What is MIR --->
				<section>
					<section>
						<h1> What is MIR?</h1>
					</section>
					<section>
						<h3 style="text-align: center;">
							<strong>
								"extending the understanding and usefulness of music data, through
								the research, development and application of computational approaches and
								tools"
							</strong>
						</h3>
						<h3 class="fragment" style="text-align: center;">
							"use of theories, concepts and techniques from
							music, computer science, signal processing and cognition"</h3>
						<div style="margin: 100px 10px 0 0 ">
							<p ><a href="http://www.nyu.edu/classes/bello/MIR_files/1-Introduction.pdf"><strong>Introduction to MIR</strong></a></p>
							<p >J. P. Bello</p>
						</div>
					</section>
					<section>
						<h2>Applications</h2>
						<h3 style="text-align: center;" class="fragment">Music recommendation<h3>
						<h3 style="text-align: center;" class="fragment">Source separation</h3>
						<h3 style="text-align: center;" class="fragment">Auto tagging</h3>
						<h3 style="text-align: center;" class="fragment">Instrument recognition</h3>
						<h3 style="text-align: center;" class="fragment">Music Transcription</h3>
					</section>
				</section>

				<!-- Traditional approaches vs Deep learning --->
				<section>
					<section>
						<h1 style="text-align: center;">Traditional approaches</h1>
					</section>
					<section>
						<h2>Hand-crafted Feature Extractors</h2>
						<h3 style="text-align: center;" class="fragment">Centroid<h3>
						<h3 style="text-align: center;" class="fragment">Rolloff</h3>
						<h3 style="text-align: center;" class="fragment">Flux</h3>
						<h3 style="text-align: center;" class="fragment">Zero Crossings</h3>
						<h3 style="text-align: center;" class="fragment">Low Energy</h3>

						<div style="margin: 100px 10px 0 0 ">
							<p ><a href="http://www.jordipons.me/apps/music-audio-tagging-at-scale-demo/"><strong>Automatic Musical Genre Classification Of Audio Signals</strong></a></p>
							<p >G. Tzanetakis, G. Essl, P. Cook 2002</p>
						</div>
					</section>
					<section>
						<h2>Classical Machine Learning</h2>
						<h3 style="text-align: center;" class="fragment">Gaussian Classifiers<h3>
						<h3 style="text-align: center;" class="fragment">Support Vector Machines</h3>
						<h3 style="text-align: center;" class="fragment">Random Forests</h3>
						<h3 style="text-align: center;" class="fragment">Clustering</h3>
					</section>
					<section>
						<h2>Enter deep learning...</h2>
					</section>
				</section>

				<!--  Applications of MIR --->
				<section>
					<section>
						<h1 style="text-align: center;">Applications</h1>
					</section>
					<section>
						<h2>Music recommendation</h2>
					</section>
					<section>
						<img src="static/ai_2/img/discover_weekly.png" style="width: 70%;">
					</section>
					<section>
						<h2>Methods</h2>
						<h3 style="text-align: center;" class="fragment">Collaborative Filtering</h2>
						<h3 style="text-align: center;" class="fragment">Natural Language Processing</h2>
						<h3 style="text-align: center;" class="fragment">Audio Content based with CNNs</h2>
					</section>
					<section>
						<h2>Source Separation</h2>
					</section>
					<section>
						<iframe width="560" height="315" src="https://www.youtube.com/embed/71WwHyNaDfE" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
						<p ><a href="https://github.com/MTG/DeepConvSep"><strong>Deep Convolutional Neural Networks for Musical Source Separation</strong></a></p>						
						<p>M. Miron, P. Chandna, G. Erruz, and H. Martel 2016</p>
					</section>
					<section>
						<iframe width="560" height="315" src="https://www.youtube.com/embed/2eVDLEQlKD0?start=30" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
						<p ><a href="http://sound-of-pixels.csail.mit.edu/"><strong>The Sound of Pixels</strong></a></p>
						<p >H. Zhao, C. Gan, A. Rouditchenko, C. Vondrick, J. McDermott, A. Torralba 2018</p>
					</section>
					<section>
						<h2>Auto Tagging</h2>
					</section>
					<section>
						<div style="margin: 10px 10px 0 0 ">
							<h4 style="text-align:center">J.S. Bach - Aria (Vergn√ºgte Ruh, Beliebte Seelenlust)</h4>
							<audio controls="">
								<source src="http://www.jordipons.me/apps/music-audio-tagging-at-scale-demo/bach_preview.mp3" type="audio/mp3">
							</audio>
							<div class="row">
								<div class="column">
									<p><strong>Top 10: Human-labels</strong></p>
									<p>female vocals, triple meter, acoustic, classical music, baroque period, lead vocals, string ensemble, major, compositional dominance of: lead vocals and melody</p>
								</div>
								<div class="column">
									<p><strong>Top 10: Deep Learning</strong></p>
									<p>acoustic, string ensemble, classical music, period baroque, major, compositional dominance of: the arrangement, form, performance, rhythm and lead vocals.</p>
								</div>
							</div>
						</div>
						<div style="margin: 200px 10px 0 0 ">
							<h4 style="text-align:center">Kendrick Lamar - Complexion (A Zulu Love)</h4>
							<audio controls="">
								<source src="http://www.jordipons.me/apps/music-audio-tagging-at-scale-demo/rap_preview.mp3" type="audio/mp3">
							</audio>
							<div class="row">
								<div class="column">
									<p><strong>Top 10: Human-labels</strong></p>
									<p>English, male vocals, rap, East Coast, breathy vocal, joyful lyrics and compositional dominance of: lyrics, melody, rhythm, accompanying vocals.</p>
								</div>
								<div class="column">
									<p><strong>Top 10: Deep Learning</strong></p>
									<p>English, lead vocals, male vocals, rap, accompanying vocals, danceable and compositional dominance of: accompanying vocals, lead vocals, rhythm, lyrics.</p>
								</div>
							</div>
						</div>
						<div style="margin: 100px 10px 0 0 ">
							<p ><a href="http://www.jordipons.me/apps/music-audio-tagging-at-scale-demo/"><strong>End-to-end learning for music audio tagging at scale</strong></a></p>
							<p >J. Pons, O. Nieto, M. Prockup, E. Schmidt, A. Ehmann and X. Serra 2017</p>
						</div>
					</section>
					<section>
						<h2>Instrument Recognition</h2>
					</section>
					<section>
						<iframe width="560" height="315" src="https://www.youtube.com/embed/FwoggcbA_Sw" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
						<div style="margin: 10px 10px 0 0 ">
							<p ><a href="https://www.izotope.com/en/blog/mixing/how-machine-learning-in-neutron-elements-track-assistant-helps-make-mixing-creative.html"><strong>Neutron Track Assistant</strong></a></p>
							<p >Gordon Wichern, iZotope 2017</p>
						</div>
					</section>
					<section>
						<h2>Music Transcription</h2>
					</section>
					<section>
						<iframe width="560" height="315" src="https://www.youtube.com/embed/g56pKqVpSrY" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
						<div style="margin: 10px 10px 0 0 ">
							<p ><a href="https://arxiv.org/abs/1710.11153"><strong>Onsets and Frames: Dual-Objective Piano Transcription</strong></a></p>
							<p >C. Hawthorne, E. Elsen, J. Song, A. Roberts, I. Simon, C. Raffel, J. Engel, S. Oore, D. Eck 2018</p>
						</div>
					</section>
				</section>

				<section>
					<div class="title">
						<h5 style="margin: 10px 0 0 0;">Next Talk - March 26</h5>
						<h1 style="margin: 25px 0 0 0;">Building Audio Plugins</h1>
						<h5 style="margin: 0 0 0 0;"> A look under the hood and getting started</h5>
					</div>
				</section>
			</div>
		</div>

		<script src="lib/js/head.min.js"></script>
		<script src="js/reveal.js"></script>
		<script src="js/slideshow.js"></script>

		<script>
			// More info about config & dependencies:
			// - https://github.com/hakimel/reveal.js#configuration
			// - https://github.com/hakimel/reveal.js#dependencies
			Reveal.initialize({
				dependencies: [
					{ src: 'plugin/markdown/marked.js' },
					{ src: 'plugin/markdown/markdown.js' },
					{ src: 'plugin/notes/notes.js', async: true },
					{ src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } }
				]
			});
		</script>
	</body>
</html>
